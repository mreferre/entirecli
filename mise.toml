[tools]
# Please also keep the version aligned in the go.mod file
go = { version = '1.25.6', postinstall = "go install github.com/go-delve/delve/cmd/dlv@latest" }
golangci-lint = '2.8.0'
shellcheck = 'latest'

[tasks.fmt]
description = "Run gofmt"
run = "gofmt -s -w ."

[tasks.test]
description = "Run tests"
run = "go test ./..."

[tasks."test:integration"]
description = "Run integration tests"
run = "go test -tags=integration ./cmd/entire/cli/integration_test/..."

[tasks."test:ci"]
description = "Run all tests (unit + integration) with race detection"
run = "go test -tags=integration -race ./..."

[tasks.build]
description = "Build the CLI"
run = """
VERSION=$(git describe --tags --always --dirty 2>/dev/null || echo "dev")
COMMIT=$(git rev-parse --short HEAD 2>/dev/null || echo "unknown")
go build -ldflags "-X github.com/entireio/cli/cmd/entire/cli/buildinfo.Version=${VERSION} -X github.com/entireio/cli/cmd/entire/cli/buildinfo.Commit=${COMMIT}" -o entire ./cmd/entire
"""

[tasks."build:all"]
description = "Build for all platforms using goreleaser"
run = "goreleaser build --snapshot --clean"

[tasks."completions"]
description = "generate entire shell completions"
quiet = true
run = """
rm -rf completions
mkdir completions
for sh in bash zsh fish; do
    go run ./cmd/entire/main.go completion "$sh" >"completions/entire.$sh"
done
"""

[tasks.dup]
description = "Check for code duplication (threshold 50, with summary)"
run = """
#!/usr/bin/env bash
set -euo pipefail

# Create temp files with proper extensions (works on both Linux and macOS)
tmpdir=$(mktemp -d)
config="$tmpdir/config.yaml"
json_out="$tmpdir/output.json"

cat > "$config" << 'YAML'
version: "2"
linters:
  default: none
  enable: [dupl]
  settings:
    dupl:
      threshold: 50
YAML

# Run with JSON output for summary, text output for details
golangci-lint run -c "$config" --new=false --max-issues-per-linter=0 --max-same-issues=0 \
  --output.json.path="$json_out" --output.text.path=/dev/stderr ./... 2>&1 || true

# Print summary grouped by file
echo ""
echo "=== Duplication Summary (by file) ==="
if command -v jq &>/dev/null && [ -s "$json_out" ]; then
  jq -r '.Issues // [] | group_by(.Pos.Filename) | map({file: (.[0].Pos.Filename | split("/") | .[-1]), count: length}) | sort_by(-.count) | .[] | "  " + (.count|tostring) + " " + .file' "$json_out" 2>/dev/null || echo "  (no issues)"
else
  echo "  (install jq for summary)"
fi

rm -rf "$tmpdir"
"""

[tasks."dup:staged"]
description = "Check duplication in staged files only (threshold 75, same as CI)"
run = """
#!/usr/bin/env bash
set -euo pipefail

# Get staged Go files, preserving paths with spaces using null delimiters throughout
if ! git diff --cached --name-only -z --diff-filter=ACM | grep -z '\\.go$' | grep -zq .; then
  echo "No staged Go files to check"
  exit 0
fi
echo "Checking staged files for duplication..."
git diff --cached --name-only -z --diff-filter=ACM | grep -z '\\.go$' | xargs -0 golangci-lint run --enable-only dupl --new=false --max-issues-per-linter=0 --max-same-issues=0
"""

[tasks.bench]
description = "Run all benchmarks"
run = '''
#!/usr/bin/env bash
set -euo pipefail

output=$(go test -bench=. -benchmem -run='^$' -timeout=10m ./... 2>&1)

# Print non-benchmark lines (ok, PASS, etc.) to stderr
echo "$output" | grep -v '^Benchmark' >&2

# Format benchmark lines as a table with headers using column
bench_lines=$(echo "$output" | grep '^Benchmark' || true)
if [ -n "$bench_lines" ]; then
  {
    echo "BENCHMARK ITERS MS/OP NS/OP B/OP ALLOCS/OP"
    echo "--------- ----- ----- ----- ---- ---------"
    # Strip unit suffixes and add ms/op column (ns/op / 1000000)
    echo "$bench_lines" | sed 's/ ns\/op//g; s/ B\/op//g; s/ allocs\/op//g; s/ transcript_bytes//g' \
      | awk '{printf "%s %s %.2f %s %s %s\n", $1, $2, $3/1000000, $3, $4, $5}'
  } | column -t
fi
'''

[tasks."bench:cpu"]
description = "Run benchmarks with CPU profile (pass package as arg, default: ./cmd/entire/cli/)"
run = '''
#!/usr/bin/env bash
set -euo pipefail

pkg="${1:-./cmd/entire/cli/}"
output=$(go test -bench=. -benchmem -run='^$' -cpuprofile=cpu.prof -timeout=10m "$pkg" 2>&1)

echo "$output" | grep -v '^Benchmark' >&2

bench_lines=$(echo "$output" | grep '^Benchmark' || true)
if [ -n "$bench_lines" ]; then
  {
    echo "BENCHMARK ITERS MS/OP NS/OP B/OP ALLOCS/OP"
    echo "--------- ----- ----- ----- ---- ---------"
    echo "$bench_lines" | sed 's/ ns\/op//g; s/ B\/op//g; s/ allocs\/op//g; s/ transcript_bytes//g' \
      | awk '{printf "%s %s %.2f %s %s %s\n", $1, $2, $3/1000000, $3, $4, $5}'
  } | column -t
fi

echo ""
echo "CPU profile saved to cpu.prof. View with: go tool pprof -http=:8080 cpu.prof"
'''

[tasks."bench:mem"]
description = "Run benchmarks with memory profile (pass package as arg, default: ./cmd/entire/cli/)"
run = '''
#!/usr/bin/env bash
set -euo pipefail

pkg="${1:-./cmd/entire/cli/}"
output=$(go test -bench=. -benchmem -run='^$' -memprofile=mem.prof -timeout=10m "$pkg" 2>&1)

echo "$output" | grep -v '^Benchmark' >&2

bench_lines=$(echo "$output" | grep '^Benchmark' || true)
if [ -n "$bench_lines" ]; then
  {
    echo "BENCHMARK ITERS MS/OP NS/OP B/OP ALLOCS/OP"
    echo "--------- ----- ----- ----- ---- ---------"
    echo "$bench_lines" | sed 's/ ns\/op//g; s/ B\/op//g; s/ allocs\/op//g; s/ transcript_bytes//g' \
      | awk '{printf "%s %s %.2f %s %s %s\n", $1, $2, $3/1000000, $3, $4, $5}'
  } | column -t
fi

echo ""
echo "Memory profile saved to mem.prof. View with: go tool pprof -http=:8080 mem.prof"
'''

[tasks."bench:compare"]
description = "Compare benchmarks between current branch and base ref"
run = """
#!/usr/bin/env bash
set -euo pipefail

BENCH_PATTERN="${BENCH_PATTERN:-.}"
BENCH_PKG="${BENCH_PKG:-./...}"
BENCH_COUNT="${BENCH_COUNT:-6}"
BENCH_TIMEOUT="${BENCH_TIMEOUT:-10m}"
BASE_REF="${BASE_REF:-main}"

current_branch=$(git rev-parse --abbrev-ref HEAD)
if [ "$current_branch" = "$BASE_REF" ]; then
  echo "Already on $BASE_REF — nothing to compare. Run from a feature branch."
  exit 1
fi

tmpdir=$(mktemp -d)
on_base_ref=false
has_changes=false

cleanup() {
  if [ "$on_base_ref" = true ]; then
    git checkout "$current_branch" --quiet 2>/dev/null || true
  fi
  if [ "$has_changes" = true ]; then
    git stash pop --quiet 2>/dev/null || true
  fi
  rm -rf "$tmpdir"
}
trap cleanup EXIT

run_bench() {
  local label="$1" out="$2"
  echo "=== Benchmarking: $label ==="
  go test -bench="$BENCH_PATTERN" -benchmem -run='^$' -count="$BENCH_COUNT" -timeout="$BENCH_TIMEOUT" "$BENCH_PKG" 2>/dev/null \
    | grep -E '^(Benchmark|goos:|goarch:|pkg:|cpu:)' > "$out"
  local count
  count=$(grep -c '^Benchmark' "$out" || true)
  if [ "$count" -eq 0 ]; then
    echo "  ERROR: no benchmark results captured. Does the benchmark exist on this branch?"
    return 1
  fi
  echo "  captured $count benchmark lines"
}

# Run on current branch first
new_out="$tmpdir/new.txt"
if ! run_bench "$current_branch" "$new_out"; then
  exit 1
fi

# Stash if needed, switch to base, run, switch back
if ! git diff --quiet || ! git diff --cached --quiet; then
  has_changes=true
  git stash push -q -m "bench:compare auto-stash"
fi

old_out="$tmpdir/old.txt"
echo ""
git checkout "$BASE_REF" --quiet 2>/dev/null
on_base_ref=true
if ! run_bench "$BASE_REF" "$old_out"; then
  echo ""
  echo "Benchmark does not exist on $BASE_REF. Showing current branch results only:"
  echo ""
  git checkout "$current_branch" --quiet 2>/dev/null
  on_base_ref=false
  [ "$has_changes" = true ] && git stash pop --quiet && has_changes=false

  # Pretty-print single-branch results (aggregate median across runs)
  python3 - "$new_out" <<'PYEOF'
import sys, statistics
from collections import defaultdict

results = defaultdict(lambda: {"ns": [], "bop": [], "allocs": []})
for line in open(sys.argv[1]):
    if not line.startswith("Benchmark"):
        continue
    parts = line.split()
    if len(parts) < 8:
        continue
    try:
        name = parts[0]
        results[name]["ns"].append(float(parts[2]))
        results[name]["bop"].append(int(parts[4]))
        results[name]["allocs"].append(int(parts[6]))
    except (ValueError, IndexError):
        continue

print(f"  {'Benchmark':<43} {'ms/op':>8} {'B/op':>8} {'allocs/op':>9}")
print(f"  {'─'*43} {'─'*8} {'─'*8} {'─'*9}")
for name, v in results.items():
    short = name.replace("BenchmarkEnableCommand/", "").replace("Benchmark", "")
    ms = statistics.median(v["ns"]) / 1e6
    bop = int(statistics.median(v["bop"]))
    allocs = int(statistics.median(v["allocs"]))
    n = len(v["ns"])
    print(f"  {short:<43} {ms:>6.1f}ms {bop:>8} {allocs:>9}  (n={n})")
PYEOF
  exit 0
fi

git checkout "$current_branch" --quiet 2>/dev/null
on_base_ref=false
[ "$has_changes" = true ] && git stash pop --quiet && has_changes=false

# Both branches have results — show comparison table (median of all runs)
echo ""
python3 - "$old_out" "$new_out" "$BASE_REF" "$current_branch" <<'PYEOF'
import sys, statistics
from collections import defaultdict

old_file, new_file, base_name, curr_name = sys.argv[1:5]

def parse_benchmarks(path):
    raw = defaultdict(lambda: {"ns": [], "bop": [], "allocs": []})
    for line in open(path):
        if not line.startswith("Benchmark"):
            continue
        parts = line.split()
        if len(parts) < 8:
            continue
        try:
            name = parts[0]
            raw[name]["ns"].append(float(parts[2]))
            raw[name]["bop"].append(int(parts[4]))
            raw[name]["allocs"].append(int(parts[6]))
        except (ValueError, IndexError):
            continue
    return {
        name: (
            statistics.median(v["ns"]) / 1e6,
            int(statistics.median(v["bop"])),
            int(statistics.median(v["allocs"])),
            len(v["ns"]),
        )
        for name, v in raw.items()
    }

old = parse_benchmarks(old_file)
new = parse_benchmarks(new_file)
all_names = list(dict.fromkeys(list(new.keys()) + list(old.keys())))

print(f"  {'Benchmark':<43} {'ms/op':>18} {'B/op':>18} {'allocs/op':>18}")
print(f"  {'─'*43}─{'─'*18}─{'─'*18}─{'─'*18}")

def fmt_delta(d):
    return f"({d:+.1f}%)"

for name in all_names:
    short = name.replace("BenchmarkEnableCommand/", "").replace("Benchmark", "")
    if name in old and name in new:
        o_ms, o_b, o_a, o_n = old[name]
        n_ms, n_b, n_a, n_n = new[name]
        d_ms = ((n_ms - o_ms) / o_ms * 100) if o_ms > 0 else 0
        d_b = ((n_b - o_b) / o_b * 100) if o_b > 0 else 0
        d_a = ((n_a - o_a) / o_a * 100) if o_a > 0 else 0
        print(f"  {short:<43} {o_ms:>6.1f} → {n_ms:>6.1f} {fmt_delta(d_ms):>7}  {o_b:>7} → {n_b:>7} {fmt_delta(d_b):>7}  {o_a:>5} → {n_a:>5} {fmt_delta(d_a):>7}")
    elif name in new:
        n_ms, n_b, n_a, n_n = new[name]
        print(f"  {short:<43} {'':>6}   {n_ms:>6.1f} {'(new)':>7}  {'':>7}   {n_b:>7} {'(new)':>7}  {'':>5}   {n_a:>5} {'(new)':>7}")

n = next((v[3] for v in new.values()), 0)
print()
print(f"  base: {base_name}    current: {curr_name}    (median of {n} runs)")
PYEOF
"""

[tasks."test:e2e"]
description = "Run E2E tests with real agent calls (requires claude CLI)"
# -count=1 disables test caching since E2E tests call real external agents
run = "go test -tags=e2e -count=1 -timeout=30m -v ./cmd/entire/cli/e2e_test/..."

[tasks."test:e2e:claude"]
description = "Run E2E tests with Claude Code (haiku model)"
run = "E2E_AGENT=claude-code go test -tags=e2e -count=1 -timeout=30m -v ./cmd/entire/cli/e2e_test/..."

[tasks."test:e2e:gemini"]
description = "Run E2E tests with Gemini CLI (sequential to avoid rate limits)"
run = "E2E_AGENT=gemini go test -tags=e2e -count=1 -parallel 1 -timeout=30m -v ./cmd/entire/cli/e2e_test/..."

[tasks."test:e2e:opencode"]
description = "Run E2E tests with OpenCode"
run = "E2E_AGENT=opencode go test -tags=e2e -count=1 -timeout=30m -v ./cmd/entire/cli/e2e_test/..."
